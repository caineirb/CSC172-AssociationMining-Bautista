{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b208f",
   "metadata": {},
   "source": [
    "## Imports and Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5318f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to install requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Set\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "import ast\n",
    "from IPython.display import display\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a7d90",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams.update({\n",
    "    \"figure.autolayout\": True,\n",
    "    \"axes.titleweight\": \"bold\"\n",
    "})\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163095e6",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a3684",
   "metadata": {},
   "source": [
    "### Process Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5061865",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Apriori analysis parameters\"\"\"\n",
    "    min_support: float = 0.05\n",
    "    min_confidence: float = 0.6\n",
    "    min_lift: float = 1.0\n",
    "    top_n_rules: int = 15\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Data processing configuration\"\"\"\n",
    "    n_workers: Optional[int] = None\n",
    "    max_memory_mb: int = 6144\n",
    "    chunk_size_multiplier: int = 4\n",
    "    base_chunk_size: int = 200_000\n",
    "    sequential_mode: bool = False\n",
    "    excluded_files: tuple = ('evaluation_unit_test_scores.csv',)    # Since this is for modeling\n",
    "    \n",
    "    def get_chunk_size(self) -> int:\n",
    "        return max(10_000, self.base_chunk_size // max(1, self.chunk_size_multiplier))\n",
    "\n",
    "# Initialize\n",
    "print(\"Initializing process configurations...\")\n",
    "analysis_config = AnalysisConfig()\n",
    "processing_config = ProcessingConfig(n_workers=4, max_memory_mb=6144)\n",
    "print(\"Configurations initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54a1c5",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage_mb() -> float:\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "\n",
    "def log_memory(context: str = \"\") -> float:\n",
    "    mem = get_memory_usage_mb()\n",
    "    print(f\"Memory {context}: {mem:.2f} MB\")\n",
    "    return mem\n",
    "\n",
    "dataframes_saves = \"../dataset/dataframes\"\n",
    "def save_dataframe(name: str, dataframe: pd.DataFrame):\n",
    "    os.makedirs(dataframes_saves, exist_ok=True)\n",
    "    # Remove any existing extension and add .parquet\n",
    "    name = os.path.splitext(name)[0] + '.parquet'\n",
    "    path = os.path.join(dataframes_saves, name)\n",
    "    dataframe.to_parquet(path, index=False, engine='pyarrow')\n",
    "\n",
    "def load_dataframe(name: str) -> pd.DataFrame:\n",
    "    # Remove any existing extension and add .parquet\n",
    "    name = os.path.splitext(name)[0] + '.parquet'\n",
    "    path = os.path.join(dataframes_saves, name)\n",
    "    return pd.read_parquet(path, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638c34f",
   "metadata": {},
   "source": [
    "### Dataset Files Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('../dataset')\n",
    "DATA_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "csv_files = sorted(DATA_FOLDER.glob('*.csv'))\n",
    "print(f\"\\nData folder: {DATA_FOLDER.absolute()}\")\n",
    "print(f\"Found {len(csv_files)} CSV files:\\n\")\n",
    "\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    excluded = f.name in processing_config.excluded_files\n",
    "    status = \"EXCLUDED\" if excluded else \"INCLUDED\"\n",
    "    print(f\"  {status} {f.name:<45} {size_mb:>10.2f} MB\")\n",
    "    if excluded:\n",
    "        csv_files.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba90f0",
   "metadata": {},
   "source": [
    "### Loading and Initial Exploration of CSV files as DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1059a8b",
   "metadata": {},
   "source": [
    "#### Initialize DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = {f_name.stem: pd.read_csv(f_name) for f_name in csv_files}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61027ea",
   "metadata": {},
   "source": [
    "#### Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_exploration(key: str, df: pd.DataFrame, head_n: int = 10):\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"{key} (shape: {df.shape})\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    report = []\n",
    "\n",
    "    n_rows = len(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Missing values\n",
    "        missing = series.isna().sum()\n",
    "\n",
    "        # Unique values (safe for lists)\n",
    "        try:\n",
    "            unique = series.nunique(dropna=True)\n",
    "        except TypeError:\n",
    "            unique = series.astype(str).nunique(dropna=True)\n",
    "\n",
    "        report.append({\n",
    "            \"Column\": col,\n",
    "            \"Type\": str(series.dtype),\n",
    "            \"Missing\": int(missing),\n",
    "            \"Missing%\": f\"{(missing / n_rows * 100):.2f}%\" if n_rows else \"0.00%\",\n",
    "            \"Unique\": int(unique),\n",
    "        })\n",
    "\n",
    "    print(\"Report:\")\n",
    "    display(pd.DataFrame(report))\n",
    "\n",
    "    # Duplicates (safe + explicit)\n",
    "    try:\n",
    "        dup_count = df.duplicated().sum()\n",
    "    except TypeError:\n",
    "        dup_count = df.astype(str).duplicated().sum()\n",
    "\n",
    "    print(f\"  Duplicates: {dup_count}\")\n",
    "    print(f\"  Sample Data:\")\n",
    "\n",
    "    # Head display (safe for large & complex columns)\n",
    "    display(df.head(head_n))\n",
    "\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    df_exploration(key, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90fdc2",
   "metadata": {},
   "source": [
    "### Data Cleaning Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b3fd2f",
   "metadata": {},
   "source": [
    "#### 'action_logs' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_ACTIONS = {\n",
    "    \"correct_response\",\n",
    "    \"wrong_response\",\n",
    "    \"answer_requested\",\n",
    "}\n",
    "\n",
    "data_frames[\"action_logs\"] = data_frames[\"action_logs\"][\n",
    "    data_frames[\"action_logs\"][\"action\"].isin(KEEP_ACTIONS)\n",
    "]\n",
    "\n",
    "# Convert timestamp to datetime (seconds → datetime)\n",
    "data_frames[\"action_logs\"][\"timestamp\"] = pd.to_datetime(\n",
    "    data_frames[\"action_logs\"][\"timestamp\"],\n",
    "    unit=\"s\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Drop unusable columns\n",
    "data_frames[\"action_logs\"] = data_frames[\"action_logs\"].drop(\n",
    "    columns=[\n",
    "        \"max_attempts\",\n",
    "        \"score_viewable\",\n",
    "        \"continuous_score_viewable\"\n",
    "    ],\n",
    "    errors=\"ignore\"  # prevents crash if column missing\n",
    ")\n",
    "\n",
    "# Sort for sequence analysis\n",
    "data_frames[\"action_logs\"] = data_frames[\"action_logs\"].sort_values(\n",
    "    [\"assignment_log_id\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Optional but HIGHLY recommended for memory\n",
    "data_frames[\"action_logs\"][\"action\"] = data_frames[\"action_logs\"][\"action\"].astype(\"category\")\n",
    "\n",
    "# Explore\n",
    "df_exploration(\"action_logs\", data_frames[\"action_logs\"])\n",
    "\n",
    "# Save cleaned dataframe\n",
    "save_dataframe(\"action_logs\", data_frames[\"action_logs\"])\n",
    "\n",
    "# Free memory\n",
    "del data_frames[\"action_logs\"]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cda7f",
   "metadata": {},
   "source": [
    "#### 'assignment_details' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time measurements\n",
    "data_frames['assignment_details'][\"assignment_start_time\"] = pd.to_datetime(\n",
    "    data_frames['assignment_details'][\"assignment_start_time\"], unit=\"s\", errors=\"coerce\"\n",
    ")\n",
    "data_frames['assignment_details'][\"assignment_end_time\"] = pd.to_datetime(\n",
    "    data_frames['assignment_details'][\"assignment_end_time\"], unit=\"s\", errors=\"coerce\"\n",
    ")\n",
    "data_frames['assignment_details'][\"assignment_release_date\"] = pd.to_datetime(\n",
    "    data_frames['assignment_details'][\"assignment_release_date\"], unit=\"s\"\n",
    ")\n",
    "data_frames['assignment_details'][\"assignment_due_date\"] = pd.to_datetime(\n",
    "    data_frames['assignment_details'][\"assignment_due_date\"], unit=\"s\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_exploration('assignment_details', data_frames['assignment_details'])\n",
    "save_dataframe('assignment_details',data_frames['assignment_details'])\n",
    "del data_frames['assignment_details']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155b0ca",
   "metadata": {},
   "source": [
    "#### 'assignment_relationships' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames['assignment_relationships'] = data_frames['assignment_relationships'].drop_duplicates()\n",
    "\n",
    "df_exploration('assignment_relationships', data_frames['assignment_relationships'])\n",
    "save_dataframe('assignment_relationships',data_frames['assignment_relationships'])\n",
    "del data_frames['assignment_relationships']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762021c",
   "metadata": {},
   "source": [
    "#### 'explanation_details' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d38631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BERT PCA vectors to embeddings\n",
    "try:\n",
    "    data_frames['explanation_details']['explanation_embedding'] = data_frames['explanation_details']['explanation_text_bert_pca'] \\\n",
    "    .apply(lambda x: np.array(ast.literal_eval(x), dtype=\"float64\"))\n",
    "\n",
    "    data_frames['explanation_details'].drop(columns=['explanation_text_bert_pca'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_exploration('explanation_details', data_frames['explanation_details'])\n",
    "save_dataframe('explanation_details',data_frames['explanation_details'])\n",
    "del data_frames['explanation_details']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7a6e7",
   "metadata": {},
   "source": [
    "#### 'hint_details' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BERT PCA vectors to embeddings\n",
    "try:\n",
    "    data_frames['hint_details']['hint_embedding'] = data_frames['hint_details']['hint_text_bert_pca'] \\\n",
    "    .apply(lambda x: np.array(ast.literal_eval(x), dtype=\"float64\"))\n",
    "\n",
    "    data_frames['hint_details'].drop(columns=['hint_text_bert_pca'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df_exploration('hint_details', data_frames['hint_details'])\n",
    "save_dataframe('hint_details',data_frames['hint_details'])\n",
    "del data_frames['hint_details']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15090c11",
   "metadata": {},
   "source": [
    "#### 'problem_details' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331aee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill boolean NA with 0\n",
    "bool_cols = [\n",
    "    \"problem_contains_image\",\n",
    "    \"problem_contains_equation\",\n",
    "    \"problem_contains_video\",\n",
    "]\n",
    "data_frames['problem_details'][bool_cols] = data_frames['problem_details'][bool_cols].fillna(0).astype(\"int8\")\n",
    "\n",
    "# Fill missing Problem Skill with Unknown\n",
    "data_frames['problem_details'][\"problem_skill_code\"] = data_frames['problem_details'][\"problem_skill_code\"].astype(\"object\").fillna(\"Unknown\").astype(\"category\")\n",
    "data_frames['problem_details'][\"problem_skill_description\"] = data_frames['problem_details'][\"problem_skill_description\"].astype(\"object\").fillna(\"Unknown\").astype(\"category\")\n",
    "\n",
    "df_exploration('problem_details', data_frames['problem_details'])\n",
    "save_dataframe('problem_details',data_frames['problem_details'])\n",
    "del data_frames['problem_details']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f02abe",
   "metadata": {},
   "source": [
    "#### 'sequence_details' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames[\"sequence_details\"][\"sequence_problem_ids\"] = (\n",
    "    data_frames[\"sequence_details\"][\"sequence_problem_ids\"]\n",
    "    .apply(lambda x: [] if not isinstance(x, str)\n",
    "           else [i.strip() for i in x.strip()[1:-1].split(\",\") if i.strip()])\n",
    ")\n",
    "\n",
    "# Fill hierarchy NaNs for grouping\n",
    "for col in data_frames['sequence_details'].columns:\n",
    "    if col.startswith(\"sequence_folder_path\"):\n",
    "        data_frames['sequence_details'][col] = data_frames['sequence_details'][col].astype('object').fillna(\"NONE\").astype('category')\n",
    "\n",
    "df_exploration('sequence_details', data_frames['sequence_details'])\n",
    "save_dataframe('sequence_details',data_frames['sequence_details'])\n",
    "del data_frames['sequence_details']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05832659",
   "metadata": {},
   "source": [
    "#### 'sequence_relationships' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8524d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames['sequence_relationships'] = data_frames['sequence_relationships'].drop_duplicates()\n",
    "\n",
    "df_exploration('sequence_relationships', data_frames['sequence_relationships'])\n",
    "save_dataframe('sequence_relationships',data_frames['sequence_relationships'])\n",
    "del data_frames['sequence_relationships']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a3f39",
   "metadata": {},
   "source": [
    "### EDA and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cabde",
   "metadata": {},
   "source": [
    "#### Action Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e73466",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808a663",
   "metadata": {},
   "source": [
    "### Analysis 1: Student-Problem Behavior Co-occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b19c5",
   "metadata": {},
   "source": [
    "#### DataFrames Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c50596",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logs = load_dataframe('action_logs')\n",
    "assignment_details = load_dataframe('assignment_details')\n",
    "problem_details = load_dataframe('problem_details')\n",
    "\n",
    "# Merge action logs with assignment details\n",
    "student_problems = action_logs.merge(\n",
    "    assignment_details, \n",
    "    on='assignment_log_id', \n",
    "    how='left'\n",
    ")\n",
    "del assignment_details\n",
    "del action_logs\n",
    "gc.collect()\n",
    "\n",
    "student_problems = student_problems[student_problems['problem_id'].notna()]\n",
    "student_problems = student_problems.sort_values(['assignment_log_id', 'problem_id', 'timestamp'])\n",
    "\n",
    "# Calculate response time difference\n",
    "student_problems['response_time'] = student_problems.groupby(\n",
    "    ['assignment_log_id', 'problem_id']\n",
    ")['timestamp'].diff()\n",
    "\n",
    "# Convert timedelta to seconds for binning\n",
    "student_problems['response_time'] = student_problems['response_time'].dt.total_seconds()\n",
    "\n",
    "bins = [-1, 10, 30, 60, 180, float(\"inf\")]\n",
    "labels = [\"very_fast\", \"fast\", \"moderate\", \"slow\", \"very_slow\"]\n",
    "\n",
    "student_problems[\"response_time_cat\"] = pd.cut(\n",
    "    student_problems[\"response_time\"],\n",
    "    bins=bins,\n",
    "    labels=labels\n",
    ").astype(\"object\").fillna(\"initial\")\n",
    "\n",
    "# Merge with problem details\n",
    "student_problems = student_problems.merge(\n",
    "    problem_details[['problem_id', 'problem_type', 'problem_skill_code']], \n",
    "    on='problem_id', \n",
    "    how='left'\n",
    ")\n",
    "print(f\"Processed {len(student_problems):,} student-problem interactions\")\n",
    "df_exploration('student_problems', student_problems)\n",
    "del problem_details\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33500c41",
   "metadata": {},
   "source": [
    "#### Transaction Encoding: Assignment-Problem Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(args):\n",
    "    \"\"\"Process a single group to create a transaction\"\"\"\n",
    "    (asg_id, prob_id), group = args\n",
    "    transaction = []\n",
    "    \n",
    "    # Add action types\n",
    "    actions = group['action'].unique()\n",
    "    transaction.extend([f\"action_{act}\" for act in actions])\n",
    "    \n",
    "    # Add response time categories\n",
    "    rt_cats = group['response_time_cat'].unique()\n",
    "    transaction.extend([f\"timing_{cat}\" for cat in rt_cats if cat != 'initial'])\n",
    "    \n",
    "    # Add problem type\n",
    "    if group['problem_type'].notna().any():\n",
    "        ptype = group['problem_type'].iloc[0]\n",
    "        transaction.append(f\"type_{ptype}\")\n",
    "    \n",
    "    # Add tutoring availability\n",
    "    if group['available_core_tutoring'].notna().any():\n",
    "        tutoring = group['available_core_tutoring'].iloc[0]\n",
    "        transaction.append(f\"tutoring_{tutoring}\")\n",
    "    \n",
    "    # Add hint usage\n",
    "    if group['hint_id'].notna().any():\n",
    "        transaction.append(\"used_hint\")\n",
    "    \n",
    "    # Add explanation usage\n",
    "    if group['explanation_id'].notna().any():\n",
    "        transaction.append(\"used_explanation\")\n",
    "    \n",
    "    return transaction if transaction else None\n",
    "\n",
    "# Create transactions in batches to manage memory\n",
    "print(\"Creating transactions in memory-efficient batches...\")\n",
    "\n",
    "# Get groupby object (doesn't load into memory yet)\n",
    "groupby_obj = student_problems.groupby(['assignment_log_id', 'problem_id'])\n",
    "total_groups = len(groupby_obj)\n",
    "print(f\"Total groups to process: {total_groups:,}\")\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 50000  # Process 50k groups at a time\n",
    "print(f\"Processing with batch size {batch_size:,}\")\n",
    "\n",
    "transactions_behaviors = []\n",
    "group_iterator = iter(groupby_obj)\n",
    "\n",
    "with tqdm(total=total_groups, desc=\"Processing transactions\") as pbar:\n",
    "    while True:\n",
    "        # Collect a batch of groups\n",
    "        batch_groups = []\n",
    "        try:\n",
    "            for _ in range(batch_size):\n",
    "                batch_groups.append(next(group_iterator))\n",
    "        except StopIteration:\n",
    "            if not batch_groups:\n",
    "                break\n",
    "        \n",
    "        # Process batch in parallel using joblib (works better with notebooks)\n",
    "        batch_results = Parallel(n_jobs=2, verbose=0, batch_size=1000)(\n",
    "            delayed(process_group)(group) for group in batch_groups\n",
    "        )\n",
    "        \n",
    "        # Filter and extend results\n",
    "        batch_results = [r for r in batch_results if r is not None]\n",
    "        transactions_behaviors.extend(batch_results)\n",
    "        \n",
    "        # Update progress\n",
    "        pbar.update(len(batch_groups))\n",
    "        \n",
    "        # Clean up batch\n",
    "        del batch_groups, batch_results\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"✓ Created {len(transactions_behaviors):,} behavior transactions\")\n",
    "\n",
    "# Free up memory before encoding\n",
    "del student_problems, groupby_obj, group_iterator\n",
    "gc.collect()\n",
    "\n",
    "# Encode transactions in batches\n",
    "print(\"Encoding transactions in batches...\")\n",
    "\n",
    "# First pass: collect all unique items efficiently using Counter\n",
    "print(\"  Pass 1: Collecting unique items...\")\n",
    "all_items_counter = Counter()\n",
    "for transaction in tqdm(transactions_behaviors, desc=\"Scanning items\"):\n",
    "    all_items_counter.update(transaction)\n",
    "\n",
    "# Sort items by frequency (most common first)\n",
    "all_items = [item for item, _ in all_items_counter.most_common()]\n",
    "print(f\"  Found {len(all_items)} unique items\")\n",
    "\n",
    "# Show top items\n",
    "print(\"\\nTop 15 Most Frequent Items:\")\n",
    "for item, count in all_items_counter.most_common(15):\n",
    "    print(f\"  {item}: {count:,}\")\n",
    "\n",
    "del all_items_counter\n",
    "gc.collect()\n",
    "\n",
    "# Create item to index mapping\n",
    "item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "# Second pass: encode in batches\n",
    "print(\"  Pass 2: Encoding transactions...\")\n",
    "encode_batch_size = 100000\n",
    "encoded_batches = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(transactions_behaviors), encode_batch_size), \n",
    "                      desc=\"Encoding batches\"):\n",
    "    end_idx = min(start_idx + encode_batch_size, len(transactions_behaviors))\n",
    "    batch = transactions_behaviors[start_idx:end_idx]\n",
    "    \n",
    "    # Manually encode batch\n",
    "    batch_encoded = np.zeros((len(batch), len(all_items)), dtype=bool)\n",
    "    for i, transaction in enumerate(batch):\n",
    "        for item in transaction:\n",
    "            batch_encoded[i, item_to_idx[item]] = True\n",
    "    \n",
    "    encoded_batches.append(batch_encoded)\n",
    "    \n",
    "    del batch\n",
    "    gc.collect()\n",
    "\n",
    "# Clear transactions_behaviors before combining\n",
    "del transactions_behaviors\n",
    "gc.collect()\n",
    "\n",
    "# Combine all batches\n",
    "print(\"Combining encoded batches...\")\n",
    "te_behaviors_array = np.vstack(encoded_batches)\n",
    "del encoded_batches\n",
    "gc.collect()\n",
    "\n",
    "# Create DataFrame\n",
    "print(\"Creating DataFrame...\")\n",
    "df_behaviors = pd.DataFrame(te_behaviors_array, columns=all_items)\n",
    "del te_behaviors_array, all_items, item_to_idx\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Transaction matrix shape: {df_behaviors.shape}\")\n",
    "print(f\"Unique items: {len(df_behaviors.columns)}\")\n",
    "\n",
    "# Show item frequencies\n",
    "print(\"Calculating item frequencies...\")\n",
    "item_freq = df_behaviors.sum().sort_values(ascending=False)\n",
    "print(\"\\nTop 15 Most Frequent Items:\")\n",
    "print(item_freq.head(15))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa32327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Apriori...\")\n",
    "frequent_itemsets_behaviors = apriori(\n",
    "    df_behaviors, \n",
    "    min_support=0.05, \n",
    "    use_colnames=True\n",
    ")\n",
    "print(f\"✓ Found {len(frequent_itemsets_behaviors)} frequent itemsets\")\n",
    "\n",
    "# Generate association rules\n",
    "rules_behaviors = association_rules(\n",
    "    frequent_itemsets_behaviors, \n",
    "    metric=\"confidence\", \n",
    "    min_threshold=0.6\n",
    ")\n",
    "rules_behaviors = rules_behaviors[rules_behaviors['lift'] >= 1.5]\n",
    "rules_behaviors = rules_behaviors.sort_values('lift', ascending=False)\n",
    "\n",
    "print(f\"✓ Generated {len(rules_behaviors)} association rules\")\n",
    "print(f\"✓ Lift range: {rules_behaviors['lift'].min():.2f} - {rules_behaviors['lift'].max():.2f}\")\n",
    "\n",
    "# Display top rules\n",
    "print(\"\\nTop 10 Rules by Lift:\\n\")\n",
    "for idx, row in rules_behaviors.head(10).iterrows():\n",
    "    antecedents = ', '.join(list(row['antecedents']))\n",
    "    consequents = ', '.join(list(row['consequents']))\n",
    "    print(f\"{antecedents} => {consequents}\")\n",
    "    print(f\"  Support: {row['support']:.3f}, Confidence: {row['confidence']:.3f}, Lift: {row['lift']:.3f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
